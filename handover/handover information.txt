handover information.txt

Some information on completed work and future recommendations:

Imputing proxy values:
As random forests cannot work with NaN values, the model currently drops andy proxies which contain NaN values within the selected time period, which is obviously very wasteful if they only have one missing value etc. So, it would be useful to investigate imputing values in proxies, which could be done a few ways:
    - Using a form of average as the default
    - Using a known external distribution
    - Combining with another proxy
    - Using a machine learning model to predict the missing value of one proxy based on others
It may then be useful to have some sort of indicator which displays 


Patterns Discovered by the Model:
The prediction generated by the model is only one part of the offering; we can also consider the patterns discovered by the model to investigate current theories about how sea ice may be related to ice cores through mechanisms such as changing wind patterns. The best way to do this would likely be using the SHAP library to explore the predictions made by the model in different output modes.


Discovering Valuable Proxies:
I also recommend using the leave-p-out style methods of predictor selection to discover which proxies might be the most 'valuable' in their contributions to model accuracy. Finding the subset of the proxies with the strongest predictive power may help inform which proxies we should spend our limited resources pursuing, or could help with evidence to aid persuasion of people who have control over access to select proxies, or may provide reassurance in ending the hunt for a proxy which is time-consuming to get our hands on but actually revealed to not be particularly important for the model.


Accumulation Years:
Adding the accumulation data to the model provided a boost in accuracy, likely because accumulation is somewhat jointly distributed with many of the chemical proxies in relation to the sea ice extent and so provides a way to distinguish variation changes in the proxy values as being caused by SIE or accumulation, and therefore excluding cases of the latter from predictions of SIE.


Neural Networks:
This type of ML model is just not suited to the task at hand, RNNs were explored but produced pretty terrible results compared to the RF.


Ridge Regression:
Ridge Regression was also explored but did not appear to yield any positive results, let alone any that would be worth the effort in implementing the method.


Loose Coupling:
As the RF model grows in complexity, and before it grows it grows too large, it may be worth trying to modularise some of the components. I would recommend putting the functions for unravelling the multiouput modes into separate scripts as a start.


Multioutput Zoning:
Currently, the model predicts on four modes: Individual, Month, Longitude, Year. It may be worth investigating different groupings that provide better predictive accuracy, such as grouping all the longitudes for a sea together for only a specific section of months.


Cross-validation:
scikit-learn does provide some out of the box cross validation, however it does not seem to have any multioutput functionality. For this reason, I think it would be best to manually implement cross-validation for the different model output versions. The approach I would take for this would be to use the np random model to split the training years into 3 random groups and then do a train test label for each of the 3 groupings and average the error across the three predictions. Note: Be wary when using the scikit-learn CV if you are using this for the 'individual' mode of output as it can cause issues based upon the fold used as the default ones provided are often not shuffled within the train/test groupings or not shuffled at all.

Hyperparameters and training data:
I did not look much into tuning hyperparameters so consider investigating this, possibly using a technique like grid search.
I expect most of the 'easy' gains will come from the hyperparameter tuning and selecting different training data; there is a tradeoff between extending the end year of the training data to widen the train set and the loss of proxies as the set is extended past their end date.


Analysing the model:
I do not believe that R-squared is a useful metric for this model due to the high variability and non-linearity of both sea extent and its relation to the proxies we are using to predict it. The maths can be slightly tricky to understand and may require some more reading if you do not have a statistics background, but a simplified version of the argument can be found here: https://stats.stackexchange.com/questions/551915/interpreting-nonlinear-regression-r2 


Units on netCDF files:
Some time could be spent updating the metadata on the netCDF files, including file and variable descriptions, as well as variable units.


Non-annual Predictors:
One briefly discussed idea was using monthly predictor values instead of yearly, as the current model uses the same annual value for a proxy regardless of the month/season being predicted, which ignores changes in proxy values throughout a year cycle and how that may relate to changes in the melt vs. freeze of sea ice. Whilst this clearly has some merit, it may be very difficult to actually form monthly versions of many of the proxies, either because we are only provided with an annual value, or because it is difficult with ice cores to tell which sample is from which part of the year. A suggested method for this sort of interpolation was to look at the accumulation climatology model to see how 'deep' in a year in the age model each month should be and then split records into months that way, or give the mid-depth of the sample as a % depth through that year's depth range. Personally, I think this may be a very difficult task with limited gains, especially as some chemical proxies such as Na have a longtail distribution and would not be well suited to this interpolation.


Outlier Years:
It could be useful to investigate treating  'low accumulation years' at a specific site as outliers to determine if MSA is just low or if it is low due to low accumulation. Removing these 'outlier' years from the training data may make the model much more accurate, but may present challenges in predicting SIE for years with low snowfall.